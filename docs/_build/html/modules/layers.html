

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorlayer.layers &mdash; TensorLayer 1.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="TensorLayer 1.1 documentation" href="../index.html"/>
        <link rel="next" title="tensorlayer.activation" href="activation.html"/>
        <link rel="prev" title="Development" href="../user/development.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> TensorLayer
          

          
          </a>

          
            
            
              <div class="version">
                1.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/development.html">Development</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#"><code class="docutils literal"><span class="pre">tensorlayer.layers</span></code></a><ul>
<li class="toctree-l2"><a class="reference internal" href="#basic-layer">Basic layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#input-layer">Input layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#word-embedding-input-layer">Word Embedding Input layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dense-layer">Dense layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#noise-layer">Noise layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#convolutional-layer">Convolutional layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#recurrent-layer">Recurrent layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#shape-layer">Shape layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#developing-or-untested">Developing or Untested</a></li>
<li class="toctree-l2"><a class="reference internal" href="#helper-functions">Helper functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="activation.html"><code class="docutils literal"><span class="pre">tensorlayer.activation</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html"><code class="docutils literal"><span class="pre">tensorlayer.nlp</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="rein.html"><code class="docutils literal"><span class="pre">tensorlayer.rein</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="iterate.html"><code class="docutils literal"><span class="pre">tensorlayer.iterate</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="cost.html"><code class="docutils literal"><span class="pre">tensorlayer.cost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="visualize.html"><code class="docutils literal"><span class="pre">tensorlayer.visualize</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="files.html"><code class="docutils literal"><span class="pre">tensorlayer.files</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html"><code class="docutils literal"><span class="pre">tensorlayer.utils</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="init.html"><code class="docutils literal"><span class="pre">tensorlayer.init</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocess.html"><code class="docutils literal"><span class="pre">tensorlayer.preprocess</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="ops.html"><code class="docutils literal"><span class="pre">tensorlayer.ops</span></code></a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">TensorLayer</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
    <li><code class="docutils literal"><span class="pre">tensorlayer.layers</span></code></li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/modules/layers.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tensorlayer-layers">
<h1><a class="reference internal" href="#module-tensorlayer.layers" title="tensorlayer.layers"><code class="xref py py-mod docutils literal"><span class="pre">tensorlayer.layers</span></code></a><a class="headerlink" href="#tensorlayer-layers" title="Permalink to this headline">¶</a></h1>
<p>To make TensorLayer simple, we minimize the number of layer classes as much as
we can. So we encourage you to use TensorFlow&#8217;s function.
For example, we do not provide layer for local response normalization, we suggest
you to apply <code class="docutils literal"><span class="pre">tf.nn.lrn</span></code> on <code class="docutils literal"><span class="pre">Layer.outputs</span></code>.</p>
<span class="target" id="module-tensorlayer.layers"></span><table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-obj docutils literal"><span class="pre">Layer</span></code></a>([inputs,&nbsp;name])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> class represents a single layer of a neural network.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#tensorlayer.layers.InputLayer" title="tensorlayer.layers.InputLayer"><code class="xref py py-obj docutils literal"><span class="pre">InputLayer</span></code></a>([inputs,&nbsp;n_features,&nbsp;name])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.InputLayer" title="tensorlayer.layers.InputLayer"><code class="xref py py-class docutils literal"><span class="pre">InputLayer</span></code></a> class is the starting layer of a neural network.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.Word2vecEmbeddingInputlayer" title="tensorlayer.layers.Word2vecEmbeddingInputlayer"><code class="xref py py-obj docutils literal"><span class="pre">Word2vecEmbeddingInputlayer</span></code></a>([inputs,&nbsp;...])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.Word2vecEmbeddingInputlayer" title="tensorlayer.layers.Word2vecEmbeddingInputlayer"><code class="xref py py-class docutils literal"><span class="pre">Word2vecEmbeddingInputlayer</span></code></a> class is a fully connected layer, for Word Embedding.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#tensorlayer.layers.EmbeddingInputlayer" title="tensorlayer.layers.EmbeddingInputlayer"><code class="xref py py-obj docutils literal"><span class="pre">EmbeddingInputlayer</span></code></a>([inputs,&nbsp;...])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.EmbeddingInputlayer" title="tensorlayer.layers.EmbeddingInputlayer"><code class="xref py py-class docutils literal"><span class="pre">EmbeddingInputlayer</span></code></a> class is a fully connected layer, for Word Embedding.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.DenseLayer" title="tensorlayer.layers.DenseLayer"><code class="xref py py-obj docutils literal"><span class="pre">DenseLayer</span></code></a>([layer,&nbsp;n_units,&nbsp;act,&nbsp;W_init,&nbsp;...])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.DenseLayer" title="tensorlayer.layers.DenseLayer"><code class="xref py py-class docutils literal"><span class="pre">DenseLayer</span></code></a> class is a fully connected layer.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#tensorlayer.layers.ReconLayer" title="tensorlayer.layers.ReconLayer"><code class="xref py py-obj docutils literal"><span class="pre">ReconLayer</span></code></a>([layer,&nbsp;x_recon,&nbsp;name,&nbsp;n_units,&nbsp;act])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.ReconLayer" title="tensorlayer.layers.ReconLayer"><code class="xref py py-class docutils literal"><span class="pre">ReconLayer</span></code></a> class is a reconstruction layer <cite>DenseLayer</cite> which use to pre-train a <cite>DenseLayer</cite>.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.DropoutLayer" title="tensorlayer.layers.DropoutLayer"><code class="xref py py-obj docutils literal"><span class="pre">DropoutLayer</span></code></a>([layer,&nbsp;keep,&nbsp;name])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.DropoutLayer" title="tensorlayer.layers.DropoutLayer"><code class="xref py py-class docutils literal"><span class="pre">DropoutLayer</span></code></a> class is a noise layer which randomly set some values to zero by a given keeping probability.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#tensorlayer.layers.DropconnectDenseLayer" title="tensorlayer.layers.DropconnectDenseLayer"><code class="xref py py-obj docutils literal"><span class="pre">DropconnectDenseLayer</span></code></a>([layer,&nbsp;keep,&nbsp;...])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.DropconnectDenseLayer" title="tensorlayer.layers.DropconnectDenseLayer"><code class="xref py py-class docutils literal"><span class="pre">DropconnectDenseLayer</span></code></a> class is <cite>DenseLayer</cite> with DropConnect behaviour which randomly remove connection between this layer to previous layer by a given keeping probability.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.Conv2dLayer" title="tensorlayer.layers.Conv2dLayer"><code class="xref py py-obj docutils literal"><span class="pre">Conv2dLayer</span></code></a>([layer,&nbsp;act,&nbsp;shape,&nbsp;strides,&nbsp;...])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.Conv2dLayer" title="tensorlayer.layers.Conv2dLayer"><code class="xref py py-class docutils literal"><span class="pre">Conv2dLayer</span></code></a> class is a 2D CNN layer.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#tensorlayer.layers.PoolLayer" title="tensorlayer.layers.PoolLayer"><code class="xref py py-obj docutils literal"><span class="pre">PoolLayer</span></code></a>([layer,&nbsp;ksize,&nbsp;strides,&nbsp;padding,&nbsp;...])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.PoolLayer" title="tensorlayer.layers.PoolLayer"><code class="xref py py-class docutils literal"><span class="pre">PoolLayer</span></code></a> class is a 2D Pooling layer.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.RNNLayer" title="tensorlayer.layers.RNNLayer"><code class="xref py py-obj docutils literal"><span class="pre">RNNLayer</span></code></a>([layer,&nbsp;cell_fn,&nbsp;cell_init_args,&nbsp;...])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.RNNLayer" title="tensorlayer.layers.RNNLayer"><code class="xref py py-class docutils literal"><span class="pre">RNNLayer</span></code></a> class is a RNN layer, you can implement vanilla RNN, LSTM and GRU with it.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#tensorlayer.layers.FlattenLayer" title="tensorlayer.layers.FlattenLayer"><code class="xref py py-obj docutils literal"><span class="pre">FlattenLayer</span></code></a>([layer,&nbsp;name])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.FlattenLayer" title="tensorlayer.layers.FlattenLayer"><code class="xref py py-class docutils literal"><span class="pre">FlattenLayer</span></code></a> class is layer which reshape each high-dimension input to a vector.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.ConcatLayer" title="tensorlayer.layers.ConcatLayer"><code class="xref py py-obj docutils literal"><span class="pre">ConcatLayer</span></code></a>([layer,&nbsp;name])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.ConcatLayer" title="tensorlayer.layers.ConcatLayer"><code class="xref py py-class docutils literal"><span class="pre">ConcatLayer</span></code></a> class is layer which concat (merge) two or more <a class="reference internal" href="#tensorlayer.layers.DenseLayer" title="tensorlayer.layers.DenseLayer"><code class="xref py py-class docutils literal"><span class="pre">DenseLayer</span></code></a> to a single class:<cite>DenseLayer</cite>.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#tensorlayer.layers.ReshapeLayer" title="tensorlayer.layers.ReshapeLayer"><code class="xref py py-obj docutils literal"><span class="pre">ReshapeLayer</span></code></a>([layer,&nbsp;shape,&nbsp;name])</td>
<td>The <a class="reference internal" href="#tensorlayer.layers.ReshapeLayer" title="tensorlayer.layers.ReshapeLayer"><code class="xref py py-class docutils literal"><span class="pre">ReshapeLayer</span></code></a> class is layer which reshape the tensor.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.flatten_reshape" title="tensorlayer.layers.flatten_reshape"><code class="xref py py-obj docutils literal"><span class="pre">flatten_reshape</span></code></a>(variable)</td>
<td>Reshapes high-dimension input to a vector.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#tensorlayer.layers.clear_layers_name" title="tensorlayer.layers.clear_layers_name"><code class="xref py py-obj docutils literal"><span class="pre">clear_layers_name</span></code></a>()</td>
<td>Clear all layer names in set_keep[&#8216;_layers_name_list&#8217;], enable layer name reuse.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.set_name_reuse" title="tensorlayer.layers.set_name_reuse"><code class="xref py py-obj docutils literal"><span class="pre">set_name_reuse</span></code></a>([enable])</td>
<td>Enable or disable reuse layer name.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#tensorlayer.layers.print_all_variables" title="tensorlayer.layers.print_all_variables"><code class="xref py py-obj docutils literal"><span class="pre">print_all_variables</span></code></a>()</td>
<td>Print all trainable and non-trainable variables</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#tensorlayer.layers.initialize_rnn_state" title="tensorlayer.layers.initialize_rnn_state"><code class="xref py py-obj docutils literal"><span class="pre">initialize_rnn_state</span></code></a>(state)</td>
<td>Return the initialized RNN state.</td>
</tr>
</tbody>
</table>
<div class="section" id="basic-layer">
<h2>Basic layer<a class="headerlink" href="#basic-layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="tensorlayer.layers.Layer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">Layer</code><span class="sig-paren">(</span><em>inputs=None</em>, <em>name='layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#Layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.Layer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> class represents a single layer of a neural network. It
should be subclassed when implementing new types of layers.
Because each layer can keep track of the layer(s) feeding into it, a
network&#8217;s output <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance can double as a handle to the full
network.</p>
<dl class="docutils">
<dt>inputs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="input-layer">
<h2>Input layer<a class="headerlink" href="#input-layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="tensorlayer.layers.InputLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">InputLayer</code><span class="sig-paren">(</span><em>inputs=None</em>, <em>n_features=None</em>, <em>name='input_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#InputLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.InputLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.InputLayer" title="tensorlayer.layers.InputLayer"><code class="xref py py-class docutils literal"><span class="pre">InputLayer</span></code></a> class is the starting layer of a neural network.</p>
<dl class="docutils">
<dt>inputs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a TensorFlow placeholder</span><dd>The input tensor data.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
<dt>n_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a int</span><dd>The number of features. If not specify, it will assume the input is
with the shape of [batch_size, n_features], then select the second
element as the n_features. It is used to specify the matrix size of
next layer. If apply Convolutional layer after InputLayer,
n_features is not important.</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="word-embedding-input-layer">
<h2>Word Embedding Input layer<a class="headerlink" href="#word-embedding-input-layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="tensorlayer.layers.Word2vecEmbeddingInputlayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">Word2vecEmbeddingInputlayer</code><span class="sig-paren">(</span><em>inputs=None</em>, <em>train_labels=None</em>, <em>vocabulary_size=80000</em>, <em>embedding_size=200</em>, <em>num_sampled=64</em>, <em>nce_loss_args={}</em>, <em>E_init=&lt;function random_uniform_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>E_init_args={}</em>, <em>nce_W_init=&lt;function truncated_normal_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>nce_W_init_args={}</em>, <em>nce_b_init=&lt;function constant_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>nce_b_init_args={}</em>, <em>name='word2vec_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#Word2vecEmbeddingInputlayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.Word2vecEmbeddingInputlayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.Word2vecEmbeddingInputlayer" title="tensorlayer.layers.Word2vecEmbeddingInputlayer"><code class="xref py py-class docutils literal"><span class="pre">Word2vecEmbeddingInputlayer</span></code></a> class is a fully connected layer,
for Word Embedding. Words are input as integer index.
The output is the embedded word vector.</p>
<dl class="docutils">
<dt>inputs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">placeholder</span><dd>For word inputs. integer index format.</dd>
<dt>train_labels</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">placeholder</span><dd>For word labels. integer index format.</dd>
<dt>vocabulary_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The size of vocabulary, number of words.</dd>
<dt>embedding_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of embedding dimensions.</dd>
<dt>num_sampled</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The Number of negative examples for NCE loss.</dd>
<dt>nce_loss_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a dictionary</span><dd>The arguments for tf.nn.nce_loss()</dd>
<dt>E_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">embedding initializer</span><dd>The initializer for initializing the embedding matrix.</dd>
<dt>E_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a dictionary</span><dd>The arguments for embedding initializer</dd>
<dt>nce_W_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">NCE decoder biases initializer</span><dd>The initializer for initializing the nce decoder weight matrix.</dd>
<dt>nce_W_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a dictionary</span><dd>The arguments for initializing the nce decoder weight matrix.</dd>
<dt>nce_b_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">NCE decoder biases initializer</span><dd>The initializer for tf.get_variable() of the nce decoder bias vector.</dd>
<dt>nce_b_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a dictionary</span><dd>The arguments for tf.get_variable() of the nce decoder bias vector.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<dl class="docutils">
<dt>nce_cost</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a tensor</span><dd>The NCE loss.</dd>
<dt>outputs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a tensor</span><dd>The outputs of embedding layer.</dd>
<dt>normalized_embeddings</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">tensor</span><dd>Normalized embedding matrix</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Without</span> <span class="n">TensorLayer</span> <span class="p">:</span> <span class="n">see</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">tutorials</span><span class="o">/</span><span class="n">word2vec</span><span class="o">/</span><span class="n">word2vec_basic</span><span class="o">.</span><span class="n">py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">train_inputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nce_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nce_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">nce_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">nce_weights</span><span class="p">,</span> <span class="n">biases</span><span class="o">=</span><span class="n">nce_biases</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">train_labels</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_sampled</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">With</span> <span class="n">TensorLayer</span> <span class="p">:</span> <span class="n">see</span> <span class="n">tutorial_word2vec_basic</span><span class="o">.</span><span class="n">py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">emb_net</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Word2vecEmbeddingInputlayer</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">train_inputs</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">vocabulary_size</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">num_sampled</span> <span class="o">=</span> <span class="n">num_sampled</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">nce_loss_args</span> <span class="o">=</span> <span class="p">{},</span>
<span class="gp">... </span>        <span class="n">E_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">E_init_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;minval&#39;</span><span class="p">:</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;maxval&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">},</span>
<span class="gp">... </span>        <span class="n">nce_W_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">nce_W_init_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;stddev&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">))},</span>
<span class="gp">... </span>        <span class="n">nce_b_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">nce_b_init_args</span> <span class="o">=</span> <span class="p">{},</span>
<span class="gp">... </span>       <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;word2vec_layer&#39;</span><span class="p">,</span>
<span class="gp">... </span>   <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cost</span> <span class="o">=</span> <span class="n">emb_net</span><span class="o">.</span><span class="n">nce_cost</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_params</span> <span class="o">=</span> <span class="n">emb_net</span><span class="o">.</span><span class="n">all_params</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
<span class="gp">... </span>                                            <span class="n">cost</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">train_params</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="n">emb_net</span><span class="o">.</span><span class="n">normalized_embeddings</span>
</pre></div>
</div>
<p><a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/word2vec/word2vec_basic.py">tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a></p>
</dd></dl>

<dl class="class">
<dt id="tensorlayer.layers.EmbeddingInputlayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">EmbeddingInputlayer</code><span class="sig-paren">(</span><em>inputs=None</em>, <em>vocabulary_size=80000</em>, <em>embedding_size=200</em>, <em>E_init=&lt;function random_uniform_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>E_init_args={}</em>, <em>name='embedding_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#EmbeddingInputlayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.EmbeddingInputlayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.EmbeddingInputlayer" title="tensorlayer.layers.EmbeddingInputlayer"><code class="xref py py-class docutils literal"><span class="pre">EmbeddingInputlayer</span></code></a> class is a fully connected layer,
for Word Embedding. Words are input as integer index.
The output is the embedded word vector.</p>
<p>This class can not be used to train a word embedding matrix, so you should
assign a trained matrix into it. To train a word embedding matrix, you can used
class:<cite>Word2vecEmbeddingInputlayer</cite>.</p>
<p>Note that, do not update this embedding matrix.</p>
<dl class="docutils">
<dt>inputs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">placeholder</span><dd>For word inputs. integer index format.
a 2D tensor : [batch_size, num_steps(num_words)]</dd>
<dt>vocabulary_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The size of vocabulary, number of words.</dd>
<dt>embedding_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of embedding dimensions.</dd>
<dt>E_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">embedding initializer</span><dd>The initializer for initializing the embedding matrix.</dd>
<dt>E_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a dictionary</span><dd>The arguments for embedding initializer</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<dl class="docutils">
<dt>outputs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a tensor</span><dd>The outputs of embedding layer.
the outputs 3D tensor : [batch_size, num_steps(num_words), embedding_size]</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_size</span> <span class="o">=</span> <span class="mi">200</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_file_name</span> <span class="o">=</span> <span class="s2">&quot;model_word2vec_50k_200&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">all_var</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">load_npy_to_any</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">model_file_name</span><span class="o">+</span><span class="s1">&#39;.npy&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">all_var</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">];</span> <span class="n">count</span> <span class="o">=</span> <span class="n">all_var</span><span class="p">[</span><span class="s1">&#39;count&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dictionary</span> <span class="o">=</span> <span class="n">all_var</span><span class="p">[</span><span class="s1">&#39;dictionary&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">all_var</span><span class="p">[</span><span class="s1">&#39;reverse_dictionary&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">save_vocab</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vocab_&#39;</span><span class="o">+</span><span class="n">model_file_name</span><span class="o">+</span><span class="s1">&#39;.txt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">del</span> <span class="n">all_var</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">count</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">load_params</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">load_npz</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">model_file_name</span><span class="o">+</span><span class="s1">&#39;.npz&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">emb_net</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">EmbeddingInputlayer</span><span class="p">(</span>
<span class="gp">... </span>               <span class="n">inputs</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
<span class="gp">... </span>               <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">vocabulary_size</span><span class="p">,</span>
<span class="gp">... </span>               <span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span><span class="p">,</span>
<span class="gp">... </span>               <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;embedding_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">assign_params</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="p">[</span><span class="n">load_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">emb_net</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word</span> <span class="o">=</span> <span class="n">b</span><span class="s1">&#39;hello&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_id</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;word_id:&#39;</span><span class="p">,</span> <span class="n">word_id</span><span class="p">)</span>
<span class="gp">... </span><span class="mi">6428</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">b</span><span class="s1">&#39;am&#39;</span><span class="p">,</span> <span class="n">b</span><span class="s1">&#39;hao&#39;</span><span class="p">,</span> <span class="n">b</span><span class="s1">&#39;dong&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_ids</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">words_to_word_ids</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">word_ids_to_words</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="n">reverse_dictionary</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;word_ids:&#39;</span><span class="p">,</span> <span class="n">word_ids</span><span class="p">)</span>
<span class="gp">... </span><span class="p">[</span><span class="mi">72</span><span class="p">,</span> <span class="mi">1226</span><span class="p">,</span> <span class="mi">46744</span><span class="p">,</span> <span class="mi">20048</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;context:&#39;</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="gp">... </span><span class="p">[</span><span class="n">b</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">b</span><span class="s1">&#39;am&#39;</span><span class="p">,</span> <span class="n">b</span><span class="s1">&#39;hao&#39;</span><span class="p">,</span> <span class="n">b</span><span class="s1">&#39;dong&#39;</span><span class="p">]</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vector</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">emb_net</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span> <span class="p">:</span> <span class="p">[</span><span class="n">word_id</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vector:&#39;</span><span class="p">,</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectors</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">emb_net</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span> <span class="p">:</span> <span class="n">word_ids</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vectors:&#39;</span><span class="p">,</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dense-layer">
<h2>Dense layer<a class="headerlink" href="#dense-layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="tensorlayer.layers.DenseLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">DenseLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>n_units=100</em>, <em>act=&lt;function relu&gt;</em>, <em>W_init=&lt;function truncated_normal_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>b_init=&lt;function constant_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>W_init_args={}</em>, <em>b_init_args={}</em>, <em>name='dense_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#DenseLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.DenseLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.DenseLayer" title="tensorlayer.layers.DenseLayer"><code class="xref py py-class docutils literal"><span class="pre">DenseLayer</span></code></a> class is a fully connected layer.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>n_units</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of units of the layer.</dd>
<dt>act</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">activation function</span><dd>The function that is applied to the layer activations.</dd>
<dt>W_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">weights initializer</span><dd>The initializer for initializing the weight matrix.</dd>
<dt>b_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">biases initializer</span><dd>The initializer for initializing the bias vector.</dd>
<dt>W_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dictionary</span><dd>The arguments for the weights tf.get_variable.</dd>
<dt>b_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dictionary</span><dd>The arguments for the biases tf.get_variable.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">W_init</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">W_init_args</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;mean&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;stddev&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">})</span>
</pre></div>
</div>
<p>If the input to this layer has more than two axes, it need to flatten the
input by using <a class="reference internal" href="#tensorlayer.layers.FlattenLayer" title="tensorlayer.layers.FlattenLayer"><code class="xref py py-class docutils literal"><span class="pre">FlattenLayer</span></code></a> in this case.</p>
</dd></dl>

<dl class="class">
<dt id="tensorlayer.layers.ReconLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">ReconLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>x_recon=None</em>, <em>name='recon_layer'</em>, <em>n_units=784</em>, <em>act=&lt;function softplus&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#ReconLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.ReconLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.ReconLayer" title="tensorlayer.layers.ReconLayer"><code class="xref py py-class docutils literal"><span class="pre">ReconLayer</span></code></a> class is a reconstruction layer <cite>DenseLayer</cite> which
use to pre-train a <cite>DenseLayer</cite>.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>x_recon</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">tensorflow variable</span><dd>The variables used for reconstruction.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
<dt>n_units</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of units of the layer, should be equal to x_recon</dd>
<dt>act</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">activation function</span><dd>The activation function that is applied to the reconstruction layer.
Normally, for sigmoid layer, the reconstruction activation is sigmoid;
for rectifying layer, the reconstruction activation is softplus.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">196</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">act</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;sigmoid1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recon_layer1</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReconLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x_recon</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">act</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;recon_layer1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recon_layer1</span><span class="o">.</span><span class="n">pretrain</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="o">=</span><span class="n">X_val</span><span class="p">,</span>
<span class="gp">... </span>                        <span class="n">denoise_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_epoch</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
<span class="gp">... </span>                        <span class="n">print_freq</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="s1">&#39;w1pre_&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="docutils">
<dt>pretrain(self, sess, x, X_train, X_val, denoise_name=None, n_epoch=100, batch_size=128, print_freq=10, save=True, save_name=&#8217;<a href="#id2"><span class="problematic" id="id3">w1pre_</span></a>&#8216;)</dt>
<dd>Start to pre-train the parameters of previous DenseLayer.</dd>
</dl>
<p>The input layer should be <cite>DenseLayer</cite> or a layer has only one axes.
You may need to modify this part to define your own cost function.
By default, the cost is implemented as follow:</p>
<p>For sigmoid layer, the implementation can be <a class="reference external" href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial">UFLDL</a></p>
<p>For rectifying layer, the implementation can be <a class="reference external" href="http://doi.org/10.1.1.208.6449">Glorot (2011). Deep Sparse Rectifier Neural Networks</a></p>
</dd></dl>

</div>
<div class="section" id="noise-layer">
<h2>Noise layer<a class="headerlink" href="#noise-layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="tensorlayer.layers.DropoutLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">DropoutLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>keep=0.5</em>, <em>name='dropout_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#DropoutLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.DropoutLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.DropoutLayer" title="tensorlayer.layers.DropoutLayer"><code class="xref py py-class docutils literal"><span class="pre">DropoutLayer</span></code></a> class is a noise layer which randomly set some
values to zero by a given keeping probability.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>keep</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The keeping probability, the lower more values will be set to zero.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">)</span>
<span class="gp">... </span><span class="n">Alternatively</span><span class="p">,</span> <span class="n">you</span> <span class="n">can</span> <span class="n">choose</span> <span class="n">a</span> <span class="n">specific</span> <span class="n">initializer</span> <span class="k">for</span> <span class="n">the</span> <span class="n">weights</span> <span class="k">as</span> <span class="n">follow</span><span class="p">:</span>
<span class="gp">... </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">,</span> <span class="n">W_init</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span> <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop3&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">identity</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output_layer&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="tensorlayer.layers.DropconnectDenseLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">DropconnectDenseLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>keep=0.5</em>, <em>n_units=100</em>, <em>act=&lt;function relu&gt;</em>, <em>W_init=&lt;function truncated_normal_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>b_init=&lt;function constant_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>W_init_args={}</em>, <em>b_init_args={}</em>, <em>name='dropconnect_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#DropconnectDenseLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.DropconnectDenseLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.DropconnectDenseLayer" title="tensorlayer.layers.DropconnectDenseLayer"><code class="xref py py-class docutils literal"><span class="pre">DropconnectDenseLayer</span></code></a> class is <cite>DenseLayer</cite> with DropConnect
behaviour which randomly remove connection between this layer to previous
layer by a given keeping probability.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>keep</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The keeping probability, the lower more values will be set to zero.</dd>
<dt>n_units</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of units of the layer.</dd>
<dt>act</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">activation function</span><dd>The function that is applied to the layer activations.</dd>
<dt>W_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">weights initializer</span><dd>The initializer for initializing the weight matrix.</dd>
<dt>b_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">biases initializer</span><dd>The initializer for initializing the bias vector.</dd>
<dt>W_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dictionary</span><dd>The arguments for the weights tf.get_variable().</dd>
<dt>b_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dictionary</span><dd>The arguments for the biases tf.get_variable().</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropconnectDenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dropconnect_relu1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropconnectDenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dropconnect_relu2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropconnectDenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">identity</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output_layer&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/papers/icml2013_wan13">Wan, L. (2013). Regularization of neural networks using dropconnect</a></p>
</dd></dl>

</div>
<div class="section" id="convolutional-layer">
<h2>Convolutional layer<a class="headerlink" href="#convolutional-layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="tensorlayer.layers.Conv2dLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">Conv2dLayer</code><span class="sig-paren">(</span><em>layer=None, act=&lt;function relu&gt;, shape=[5, 5, 1, 100], strides=[1, 1, 1, 1], padding='SAME', W_init=&lt;function truncated_normal_initializer.&lt;locals&gt;._initializer&gt;, b_init=&lt;function constant_initializer.&lt;locals&gt;._initializer&gt;, W_init_args={}, b_init_args={}, name='cnn_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#Conv2dLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.Conv2dLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.Conv2dLayer" title="tensorlayer.layers.Conv2dLayer"><code class="xref py py-class docutils literal"><span class="pre">Conv2dLayer</span></code></a> class is a 2D CNN layer.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>act</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">activation function</span><dd>The function that is applied to the layer activations.</dd>
<dt>n_units</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of units of the layer</dd>
<dt>shape</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of shape</span><dd>shape of the filters</dd>
<dt>strides</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a list of ints. 1-D of length 4.</span><dd><p class="first">The stride of the sliding window for each dimension of input.</p>
<p class="last">It Must be in the same order as the dimension specified with format.</p>
</dd>
<dt>padding</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string from: &#8220;SAME&#8221;, &#8220;VALID&#8221;.</span><dd>The type of padding algorithm to use.</dd>
<dt>W_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">weights initializer</span><dd>The initializer for initializing the weight matrix.</dd>
<dt>b_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">biases initializer</span><dd>The initializer for initializing the bias vector.</dd>
<dt>W_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dictionary</span><dd>The arguments for the weights tf.get_variable().</dd>
<dt>b_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dictionary</span><dd>The arguments for the biases tf.get_variable().</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2dLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>  <span class="c1"># 32 features for each 5x5 patch</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">W_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">W_init_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;mean&#39;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;stddev&#39;</span><span class="p">:</span><span class="mi">3</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">b_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">b_init_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;name&#39;</span> <span class="p">:</span> <span class="s1">&#39;bias&#39;</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;cnn_layer1&#39;</span><span class="p">)</span>     <span class="c1"># output: (?, 28, 28, 32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">PoolLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
<span class="gp">... </span>                  <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
<span class="gp">... </span>                  <span class="n">pool</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">,</span>
<span class="gp">... </span>                  <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;pool_layer1&#39;</span><span class="p">,)</span>   <span class="c1"># output: (?, 14, 14, 32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="tensorlayer.layers.PoolLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">PoolLayer</code><span class="sig-paren">(</span><em>layer=None, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', pool=&lt;function max_pool&gt;, name='pool_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#PoolLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.PoolLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.PoolLayer" title="tensorlayer.layers.PoolLayer"><code class="xref py py-class docutils literal"><span class="pre">PoolLayer</span></code></a> class is a 2D Pooling layer.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>ksize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a list of ints that has length &gt;= 4.</span><dd>The size of the window for each dimension of the input tensor.</dd>
<dt>strides</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a list of ints that has length &gt;= 4.</span><dd>The stride of the sliding window for each dimension of the input tensor.</dd>
<dt>padding</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string from: &#8220;SAME&#8221;, &#8220;VALID&#8221;.</span><dd>The type of padding algorithm to use.</dd>
<dt>pool</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a pooling function</span><dd>tf.nn.max_pool , tf.nn.avg_pool ...</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<p>see Conv2dLayer</p>
<p><a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#pooling">TensorFlow Pooling</a></p>
</dd></dl>

</div>
<div class="section" id="recurrent-layer">
<h2>Recurrent layer<a class="headerlink" href="#recurrent-layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="tensorlayer.layers.RNNLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">RNNLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>cell_fn=&lt;class 'tensorflow.python.ops.rnn_cell.BasicRNNCell'&gt;</em>, <em>cell_init_args={}</em>, <em>n_hidden=100</em>, <em>initializer=&lt;function random_uniform_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>n_steps=5</em>, <em>return_last=False</em>, <em>return_seq_2d=False</em>, <em>name='rnn_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#RNNLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.RNNLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.RNNLayer" title="tensorlayer.layers.RNNLayer"><code class="xref py py-class docutils literal"><span class="pre">RNNLayer</span></code></a> class is a RNN layer, you can implement vanilla RNN,
LSTM and GRU with it.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>cell_fn</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a TensorFlow&#8217;s core RNN cell as follow.</span><dd><p class="first">see <a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/rnn_cell.html">RNN Cells in TensorFlow</a></p>
<p>class tf.nn.rnn_cell.BasicRNNCell</p>
<p>class tf.nn.rnn_cell.BasicLSTMCell</p>
<p>class tf.nn.rnn_cell.GRUCell</p>
<p class="last">class tf.nn.rnn_cell.LSTMCell</p>
</dd>
<dt>cell_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a dictionary</span><dd>The arguments for the cell initializer.</dd>
<dt>n_hidden</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a int</span><dd>The number of hidden units in the layer.</dd>
<dt>n_steps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a int</span><dd>The sequence length.</dd>
<dt>return_last</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolen</span><dd><p class="first">If True, return the last output, &#8220;Sequence input and single output&#8221;</p>
<p>If False, return all outputs, &#8220;Synced sequence input and output&#8221;</p>
<p class="last">In other word, if you want to apply one or more RNN(s) on this layer, set to False.</p>
</dd>
</dl>
<p># is_reshape : boolen (deprecate)
#     Reshape the inputs to 3 dimension tensor.</p>
<p>#     If input is［batch_size, n_steps, n_features], we do not need to reshape it.</p>
<p>#     If input is [batch_size * n_steps, n_features], we need to reshape it.
return_seq_2d : boolen</p>
<blockquote>
<div><p>When return_last = False</p>
<blockquote>
<div>if True, return 2D Tensor [n_example, n_hidden], for stacking DenseLayer after it.
if False, return 3D Tensor [n_example/n_steps, n_steps, n_hidden], for stacking multiple RNN after it.</div></blockquote>
</div></blockquote>
<dl class="docutils">
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<dl class="docutils">
<dt>outputs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a tensor</span><dd><p class="first">The output of this RNN.
return_last = False, outputs = all cell_output, which is the hidden state.</p>
<blockquote class="last">
<div>cell_output.get_shape() = (?, n_hidden)</div></blockquote>
</dd>
<dt>final_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a tensor or StateTuple</span><dd><p class="first">When state_is_tuple = False,
it is the final hidden and cell states, states.get_shape() = [?, 2 * n_hidden].</p>
<p class="last">When state_is_tuple = True, it stores two elements: (c, h), in that order.
You can get the final state after each iteration during training, then
feed it to the initial state of next iteration.</p>
</dd>
<dt>initial_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a tensor or StateTuple</span><dd>It is the initial state of this RNN layer, you can use it to initialize
your state at the begining of each epoch or iteration according to your
training procedure.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go"># ... For single RNN</span>
<span class="go"># &gt;&gt;&gt; network =</span>
<span class="go"># ...</span>
<span class="go"># ...</span>
<span class="go"># ... For multiple RNNs</span>
<span class="go"># &gt;&gt;&gt; network =</span>
<span class="go">#</span>
</pre></div>
</div>
<p>If the input to this layer has more than two axes, we need to flatten the
input by using <a class="reference internal" href="#tensorlayer.layers.FlattenLayer" title="tensorlayer.layers.FlattenLayer"><code class="xref py py-class docutils literal"><span class="pre">FlattenLayer</span></code></a>.</p>
<p><a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/rnn_cell.html">Neural Network RNN Cells in TensorFlow</a></p>
<p><a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py">tensorflow/python/ops/rnn.py</a></p>
<p><a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py">tensorflow/python/ops/rnn_cell.py</a></p>
<p>see TensorFlow tutorial <code class="docutils literal"><span class="pre">ptb_word_lm.py</span></code></p>
</dd></dl>

</div>
<div class="section" id="shape-layer">
<h2>Shape layer<a class="headerlink" href="#shape-layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="tensorlayer.layers.FlattenLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">FlattenLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>name='flatten_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#FlattenLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.FlattenLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.FlattenLayer" title="tensorlayer.layers.FlattenLayer"><code class="xref py py-class docutils literal"><span class="pre">FlattenLayer</span></code></a> class is layer which reshape each high-dimension
input to a vector. Then we can apply DenseLayer, RNNLayer, ConcatLayer and
etc on the top of it.</p>
<p>[batch_size, mask_row, mask_col, n_mask] &#8212;&gt; [batch_size, mask_row * mask_col * n_mask]</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2dLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
<span class="gp">... </span>                   <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
<span class="gp">... </span>                   <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
<span class="gp">... </span>                   <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;cnn_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Pool2dLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
<span class="gp">... </span>                   <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
<span class="gp">... </span>                   <span class="n">pool</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">,</span>
<span class="gp">... </span>                   <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;pool_layer&#39;</span><span class="p">,)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">FlattenLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;flatten_layer&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="tensorlayer.layers.ConcatLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">ConcatLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>name='concat_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#ConcatLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.ConcatLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.ConcatLayer" title="tensorlayer.layers.ConcatLayer"><code class="xref py py-class docutils literal"><span class="pre">ConcatLayer</span></code></a> class is layer which concat (merge) two or more
<a class="reference internal" href="#tensorlayer.layers.DenseLayer" title="tensorlayer.layers.DenseLayer"><code class="xref py py-class docutils literal"><span class="pre">DenseLayer</span></code></a> to a single class:<cite>DenseLayer</cite>.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a list of <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instances</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span>&gt;&gt;&gt; sess = tf.InteractiveSession()
&gt;&gt;&gt; x = tf.placeholder(tf.float32, shape=[None, 784])
&gt;&gt;&gt; inputs = tl.layers.InputLayer(x, name=&#39;input_layer&#39;)
&gt;&gt;&gt; net1 = tl.layers.DenseLayer(inputs, n_units=800, act = tf.nn.relu, name=&#39;relu1_1&#39;)
&gt;&gt;&gt; net2 = tl.layers.DenseLayer(inputs, n_units=300, act = tf.nn.relu, name=&#39;relu2_1&#39;)
&gt;&gt;&gt; network = tl.layers.ConcatLayer(layer = [net1, net2], name =&#39;concat_layer&#39;)
...     tensorlayer:Instantiate InputLayer input_layer (?, 784)
...     tensorlayer:Instantiate DenseLayer relu1_1: 800, &lt;function relu at 0x1108e41e0&gt;
...     tensorlayer:Instantiate DenseLayer relu2_1: 300, &lt;function relu at 0x1108e41e0&gt;
...     tensorlayer:Instantiate ConcatLayer concat_layer, 1100
...
&gt;&gt;&gt; sess.run(tf.initialize_all_variables())
&gt;&gt;&gt; network.print_params()
...     param 0: (784, 800) (mean: 0.000021, median: -0.000020 std: 0.035525)
...     param 1: (800,) (mean: 0.000000, median: 0.000000 std: 0.000000)
...     param 2: (784, 300) (mean: 0.000000, median: -0.000048 std: 0.042947)
...     param 3: (300,) (mean: 0.000000, median: 0.000000 std: 0.000000)
...     num of params: 863500
&gt;&gt;&gt; network.print_layers()
...     layer 0: Tensor(&quot;Relu:0&quot;, shape=(?, 800), dtype=float32)
...     layer 1: Tensor(&quot;Relu_1:0&quot;, shape=(?, 300), dtype=float32)
...
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="tensorlayer.layers.ReshapeLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">ReshapeLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>shape=[]</em>, <em>name='reshape_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#ReshapeLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.ReshapeLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#tensorlayer.layers.ReshapeLayer" title="tensorlayer.layers.ReshapeLayer"><code class="xref py py-class docutils literal"><span class="pre">ReshapeLayer</span></code></a> class is layer which reshape the tensor.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>shape</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a list</span><dd>The output shape.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<p>... The core of this layer is <code class="docutils literal"><span class="pre">tf.reshape</span></code>.
... Use TensorFlow only:
&gt;&gt;&gt; x = tf.placeholder(tf.float32, shape=[None, 3])
&gt;&gt;&gt; y = tf.reshape(x, shape=[-1, 3, 3])
&gt;&gt;&gt; sess = tf.InteractiveSession()
&gt;&gt;&gt; print(sess.run(y, feed_dict={x:[[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5],[6,6,6]]}))
... [[[ 1.  1.  1.]
... [ 2.  2.  2.]
... [ 3.  3.  3.]]
... [[ 4.  4.  4.]
... [ 5.  5.  5.]
... [ 6.  6.  6.]]]</p>
</dd></dl>

</div>
<div class="section" id="developing-or-untested">
<h2>Developing or Untested<a class="headerlink" href="#developing-or-untested" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="tensorlayer.layers.Conv3dLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">Conv3dLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>act=&lt;function relu&gt;</em>, <em>shape=[]</em>, <em>strides=[]</em>, <em>padding='SAME'</em>, <em>W_init=&lt;function truncated_normal_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>b_init=&lt;function constant_initializer.&lt;locals&gt;._initializer&gt;</em>, <em>W_init_args={}</em>, <em>b_init_args={}</em>, <em>name='cnn_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#Conv3dLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.Conv3dLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Untested</p>
<p>The <a class="reference internal" href="#tensorlayer.layers.Conv3dLayer" title="tensorlayer.layers.Conv3dLayer"><code class="xref py py-class docutils literal"><span class="pre">Conv3dLayer</span></code></a> class is a 3D CNN layer.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>act</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">activation function</span><dd>The function that is applied to the layer activations.</dd>
<dt>n_units</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of units of the layer</dd>
<dt>shape</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of shape</span><dd>shape of the filters</dd>
<dt>strides</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a list of ints. 1-D of length 4.</span><dd>The stride of the sliding window for each dimension of input. Must be in the same order as the dimension specified with format.</dd>
<dt>padding</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string from: &#8220;SAME&#8221;, &#8220;VALID&#8221;.</span><dd>The type of padding algorithm to use.</dd>
<dt>W_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">weights initializer</span><dd>The initializer for initializing the weight matrix.</dd>
<dt>b_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">biases initializer</span><dd>The initializer for initializing the bias vector.</dd>
<dt>W_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dictionary</span><dd>The arguments for the weights initializer.</dd>
<dt>b_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dictionary</span><dd>The arguments for the biases initializer.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<p><a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#conv3d">tf.nn.conv3d</a></p>
</dd></dl>

<dl class="class">
<dt id="tensorlayer.layers.MaxoutLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">MaxoutLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>n_units=100</em>, <em>name='maxout_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#MaxoutLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.MaxoutLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Coming soon</p>
<p>Single DenseLayer with Max-out behaviour, work well with Dropout.</p>
<p><a class="reference external" href="http://arxiv.org/abs/1302.4389">Goodfellow (2013) Maxout Networks</a></p>
</dd></dl>

<dl class="class">
<dt id="tensorlayer.layers.GaussianNoiseLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">GaussianNoiseLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>name='gaussian_noise_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#GaussianNoiseLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.GaussianNoiseLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Coming soon</p>
</dd></dl>

<dl class="class">
<dt id="tensorlayer.layers.BidirectionalRNNLayer">
<em class="property">class </em><code class="descclassname">tensorlayer.layers.</code><code class="descname">BidirectionalRNNLayer</code><span class="sig-paren">(</span><em>layer=None</em>, <em>n_hidden=100</em>, <em>n_steps=5</em>, <em>return_last=False</em>, <em>is_reshape=True</em>, <em>cell_init_args={'forget_bias': 1.0}</em>, <em>name='basic_lstm_layer'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#BidirectionalRNNLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.BidirectionalRNNLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Coming soon</p>
<p>The <a class="reference internal" href="#tensorlayer.layers.BidirectionalRNNLayer" title="tensorlayer.layers.BidirectionalRNNLayer"><code class="xref py py-class docutils literal"><span class="pre">BidirectionalRNNLayer</span></code></a> class is a RNN layer.</p>
<dl class="docutils">
<dt>layer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a <a class="reference internal" href="#tensorlayer.layers.Layer" title="tensorlayer.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">Layer</span></code></a> instance</span><dd>The <cite>Layer</cite> class feeding into this layer.</dd>
<dt>n_hidden</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a int</span><dd>The number of hidden units in the layer.</dd>
<dt>n_steps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a int</span><dd>The sequence length.</dd>
<dt>return_last</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolen</span><dd>If True, return the last output, &#8220;Sequence input and single output&#8221;
If False, return all outputs, &#8220;Synced sequence input and output&#8221;
In other word, if you want to apply one or more RNN(s) on this layer, set to False.</dd>
<dt>cell_init_args</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a dictionary</span><dd>The arguments for the cell initializer.</dd>
<dt>is_reshape</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolen</span><dd>Reshape the inputs to 3 dimension tensor.
If input is［batch_size, n_steps, n_features], we do not need to reshape it.
If input is [batch_size * n_steps, n_features], we need to reshape it.</dd>
<dt>name</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a string or None</span><dd>An optional name to attach to this layer.</dd>
</dl>
<dl class="docutils">
<dt>outputs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a tensor</span><dd>The output of this RNN.</dd>
<dt>state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">a tensor</span><dd>When state_is_tuple=False
It is the final hidden and cell states, states.get_shape() = [?, 2 * n_hidden]</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<p><a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/rnn_cell.html">Neural Network RNN Cells in TensorFlow</a></p>
</dd></dl>

</div>
<div class="section" id="helper-functions">
<h2>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="tensorlayer.layers.flatten_reshape">
<code class="descclassname">tensorlayer.layers.</code><code class="descname">flatten_reshape</code><span class="sig-paren">(</span><em>variable</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#flatten_reshape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.flatten_reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshapes high-dimension input to a vector.
[batch_size, mask_row, mask_col, n_mask] &#8212;&gt; [batch_size, mask_row * mask_col * n_mask]</p>
<p>variable : a tensorflow variable</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">W_conv2</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>   <span class="c1"># 64 features for each 5x5 patch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b_conv2</span> <span class="o">=</span> <span class="n">bias_variable</span><span class="p">([</span><span class="mi">32</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W_fc1</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">([</span><span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">256</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">h_conv2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">conv2d</span><span class="p">(</span><span class="n">h_pool1</span><span class="p">,</span> <span class="n">W_conv2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_conv2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h_pool2</span> <span class="o">=</span> <span class="n">max_pool_2x2</span><span class="p">(</span><span class="n">h_conv2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h_pool2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[:]</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>
<span class="gp">... </span>        <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">mask_row</span><span class="p">,</span> <span class="n">mask_col</span><span class="p">,</span> <span class="n">n_mask</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h_pool2_flat</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">flatten_reshape</span><span class="p">(</span><span class="n">h_pool2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">mask_row</span> <span class="o">*</span> <span class="n">mask_col</span> <span class="o">*</span> <span class="n">n_mask</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h_pool2_flat_drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">h_pool2_flat</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
<span class="gp">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="tensorlayer.layers.clear_layers_name">
<code class="descclassname">tensorlayer.layers.</code><code class="descname">clear_layers_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#clear_layers_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.clear_layers_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear all layer names in set_keep[&#8216;_layers_name_list&#8217;],
enable layer name reuse.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">clear_layers_name</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network2</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network2</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">)</span>
<span class="gp">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="tensorlayer.layers.set_name_reuse">
<code class="descclassname">tensorlayer.layers.</code><code class="descname">set_name_reuse</code><span class="sig-paren">(</span><em>enable=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#set_name_reuse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.set_name_reuse" title="Permalink to this definition">¶</a></dt>
<dd><p>Enable or disable reuse layer name. By default, each layer must has unique
name. When you want two or more input placeholder (inference) share the same
model parameters, you need to enable layer name reuse, then allow the
parameters have same name scope.</p>
<p>see <code class="docutils literal"><span class="pre">tutorial_ptb_lstm.py</span></code> for example.</p>
</dd></dl>

<dl class="function">
<dt id="tensorlayer.layers.print_all_variables">
<code class="descclassname">tensorlayer.layers.</code><code class="descname">print_all_variables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#print_all_variables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.print_all_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Print all trainable and non-trainable variables
without initialize_all_variables()</p>
</dd></dl>

<dl class="function">
<dt id="tensorlayer.layers.initialize_rnn_state">
<code class="descclassname">tensorlayer.layers.</code><code class="descname">initialize_rnn_state</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tensorlayer/layers.html#initialize_rnn_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorlayer.layers.initialize_rnn_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the initialized RNN state.
The input is LSTMStateTuple or State of RNNCells.</p>
</dd></dl>

</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="activation.html" class="btn btn-neutral float-right" title="tensorlayer.activation" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../user/development.html" class="btn btn-neutral" title="Development" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, TensorLayer contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>