{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial MNIST Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the lib and start the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`InteractiveSession` installs itself as the default session on construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load or Download MNIST > data/mnist/\n",
      "data/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading train-labels-idx1-ubyte.gz...113%\n",
      "Succesfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Downloading t10k-images-idx3-ubyte.gz...100%\n",
      "Succesfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading t10k-labels-idx1-ubyte.gz...180%\n",
      "Succesfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = \\\n",
    "                                tl.files.load_mnist_dataset(shape=(-1,784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define placeholder\n",
    "\n",
    "Since we have to train our example, so we have to build a placeholder.\n",
    "\n",
    "The `tf.placeholder` is used to feed actual training examples.\n",
    "\n",
    "[For more information](https://www.tensorflow.org/programmers_guide/reading_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
    "y_ = tf.placeholder(tf.int64, shape=[None, ], name='y_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`InputLayer` class is the starting layer of a neural network. The first thing that we to do is to get our network prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] InputLayer  input_layer: (?, 784)\n"
     ]
    }
   ],
   "source": [
    "network = tl.layers.InputLayer(x, name='input_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name is an optional name to attach to this layer. But it would be better if you feed the words.\n",
    "\n",
    "Now, we use TensorLayer to build the Models.\n",
    "\n",
    "This is where TensorLayer steps in. It allows you to define an arbitrarily structured neural network by creating and stacking or merging layers. Since every layer knows its immediate incoming layers, the output layer (or output layers) of a network double as a handle to the network as a whole, so usually this is the only thing we will pass on to the rest of the code.\n",
    "\n",
    "`tutorial_mnist_simple.py` is a simple example for MNIST dataset.\n",
    "\n",
    "Here we don't talk much about the layers that we made here, if you are interest in these, please do go the [TensorLayer API - Layer](https://tensorlayer.readthedocs.io/en/latest/modules/layers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] DropoutLayer drop1: keep:0.800000 is_fix:False\n",
      "  [TL] DenseLayer  relu1: 800 relu\n",
      "  [TL] DropoutLayer drop2: keep:0.500000 is_fix:False\n",
      "  [TL] DenseLayer  relu2: 800 relu\n",
      "  [TL] DropoutLayer drop3: keep:0.500000 is_fix:False\n"
     ]
    }
   ],
   "source": [
    "network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n",
    "network = tl.layers.DenseLayer(network, n_units=800,\n",
    "                                act = tf.nn.relu, name='relu1')\n",
    "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')\n",
    "network = tl.layers.DenseLayer(network, n_units=800,\n",
    "                                act = tf.nn.relu, name='relu2')\n",
    "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax is implemented internally in tl.cost.cross_entropy(y, y_) to speed up computation, so we use identity here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] DenseLayer  output_layer: 10 identity\n"
     ]
    }
   ],
   "source": [
    "network = tl.layers.DenseLayer(network, n_units=10,\n",
    "                                act = tf.identity,\n",
    "                                name='output_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cost function and metric.\n",
    "\n",
    "`tl.cost.cross_entropy()` is a softmax cross-entropy operation, returns the TensorFlow expression of cross-entropy of two distributions, implement softmax internally.\n",
    "\n",
    "With the given Tensorflow variable(in this scenario, is `y` and `y_`)\n",
    "\n",
    "About cross-entropy [WikiPedia](https://en.wikipedia.org/wiki/Cross_entropy) \n",
    "\n",
    "Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck, but not both.\n",
    "\n",
    "And the `equal` will return the truth value of (x == y) element-wise.Will return a `Tensor` of type `bool`.\n",
    "\n",
    "Using all of these method, we could define cost function and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = network.outputs\n",
    "cost = tl.cost.cross_entropy(y, y_, name='xentropy')\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "y_op = tf.argmax(tf.nn.softmax(y), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer\n",
    "\n",
    "`tf.train.AdamOptimizer())`is using an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\n",
    "\n",
    "* learning_rate: A Tensor or a floating point value.  The learning rate.\n",
    "* beta1: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.\n",
    "* beta2: A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates.\n",
    "* epsilon: A small constant for numerical stability.\n",
    "* use_locking: If True use locks for update operations.\n",
    "* name: Optional name for the operations created when applying gradients.(Defaults to \"Adam\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_params = network.all_params\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0.9, beta2=0.999,\n",
    "                                  epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize all variables in the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tl.layers.initialize_global_variables(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print network information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param   0: (784, 800)      (mean: -7.893177826190367e-05, median: -0.00025377183919772506, std: 0.08798402547836304)   relu1/W:0\n",
      "  param   1: (800,)          (mean: 0.0               , median: 0.0               , std: 0.0               )   relu1/b:0\n",
      "  param   2: (800, 800)      (mean: 2.5794106477405876e-05, median: -2.705508904909948e-06, std: 0.08786776661872864)   relu2/W:0\n",
      "  param   3: (800,)          (mean: 0.0               , median: 0.0               , std: 0.0               )   relu2/b:0\n",
      "  param   4: (800, 10)       (mean: 0.00037536685704253614, median: -7.521080988226458e-05, std: 0.08868692070245743)   output_layer/W:0\n",
      "  param   5: (10,)           (mean: 0.0               , median: 0.0               , std: 0.0               )   output_layer/b:0\n",
      "  num of params: 1276810\n"
     ]
    }
   ],
   "source": [
    "network.print_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the layers that we have build for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  layer 0: Tensor(\"drop1/mul:0\", shape=(?, 784), dtype=float32)\n",
      "  layer 1: Tensor(\"relu1/Relu:0\", shape=(?, 800), dtype=float32)\n",
      "  layer 2: Tensor(\"drop2/mul:0\", shape=(?, 800), dtype=float32)\n",
      "  layer 3: Tensor(\"relu2/Relu:0\", shape=(?, 800), dtype=float32)\n",
      "  layer 4: Tensor(\"drop3/mul:0\", shape=(?, 800), dtype=float32)\n",
      "  layer 5: Tensor(\"output_layer/Identity:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "network.print_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the network ...\n",
      "Epoch 1 of 500 took 2.163315s\n",
      "   val loss: 0.553532\n",
      "   val acc: 0.825900\n",
      "Epoch 5 of 500 took 1.254972s\n",
      "   val loss: 0.282738\n",
      "   val acc: 0.917800\n",
      "Epoch 10 of 500 took 1.169460s\n",
      "   val loss: 0.222307\n",
      "   val acc: 0.937500\n",
      "Epoch 15 of 500 took 1.272432s\n",
      "   val loss: 0.187691\n",
      "   val acc: 0.947200\n",
      "Epoch 20 of 500 took 1.416087s\n",
      "   val loss: 0.163059\n",
      "   val acc: 0.954700\n",
      "Epoch 25 of 500 took 1.381155s\n",
      "   val loss: 0.146343\n",
      "   val acc: 0.960100\n",
      "Epoch 30 of 500 took 1.258913s\n",
      "   val loss: 0.131556\n",
      "   val acc: 0.963100\n",
      "Epoch 35 of 500 took 1.570845s\n",
      "   val loss: 0.121536\n",
      "   val acc: 0.966500\n",
      "Epoch 40 of 500 took 1.439608s\n",
      "   val loss: 0.113252\n",
      "   val acc: 0.969300\n",
      "Epoch 45 of 500 took 1.432343s\n",
      "   val loss: 0.104831\n",
      "   val acc: 0.970900\n",
      "Epoch 50 of 500 took 1.372524s\n",
      "   val loss: 0.099427\n",
      "   val acc: 0.971600\n",
      "Epoch 55 of 500 took 1.463206s\n",
      "   val loss: 0.094483\n",
      "   val acc: 0.972800\n",
      "Epoch 60 of 500 took 1.408511s\n",
      "   val loss: 0.090017\n",
      "   val acc: 0.973500\n",
      "Epoch 65 of 500 took 1.298293s\n",
      "   val loss: 0.086689\n",
      "   val acc: 0.974700\n",
      "Epoch 70 of 500 took 1.298469s\n",
      "   val loss: 0.082902\n",
      "   val acc: 0.975800\n",
      "Epoch 75 of 500 took 1.342089s\n",
      "   val loss: 0.079527\n",
      "   val acc: 0.976900\n",
      "Epoch 80 of 500 took 1.883392s\n",
      "   val loss: 0.077401\n",
      "   val acc: 0.977000\n",
      "Epoch 85 of 500 took 1.672204s\n",
      "   val loss: 0.075225\n",
      "   val acc: 0.979000\n",
      "Epoch 90 of 500 took 1.484444s\n",
      "   val loss: 0.073304\n",
      "   val acc: 0.979000\n",
      "Epoch 95 of 500 took 1.533131s\n",
      "   val loss: 0.070630\n",
      "   val acc: 0.979800\n",
      "Epoch 100 of 500 took 1.395635s\n",
      "   val loss: 0.069117\n",
      "   val acc: 0.980500\n",
      "Epoch 105 of 500 took 1.396828s\n",
      "   val loss: 0.067845\n",
      "   val acc: 0.980600\n",
      "Epoch 110 of 500 took 1.425587s\n",
      "   val loss: 0.066682\n",
      "   val acc: 0.980800\n",
      "Epoch 115 of 500 took 1.503976s\n",
      "   val loss: 0.066283\n",
      "   val acc: 0.981100\n",
      "Epoch 120 of 500 took 1.496569s\n",
      "   val loss: 0.064150\n",
      "   val acc: 0.981400\n",
      "Epoch 125 of 500 took 1.380668s\n",
      "   val loss: 0.063254\n",
      "   val acc: 0.982100\n",
      "Epoch 130 of 500 took 1.559914s\n",
      "   val loss: 0.062878\n",
      "   val acc: 0.982200\n",
      "Epoch 135 of 500 took 1.354967s\n",
      "   val loss: 0.062238\n",
      "   val acc: 0.982900\n",
      "Epoch 140 of 500 took 1.387792s\n",
      "   val loss: 0.060890\n",
      "   val acc: 0.982500\n",
      "Epoch 145 of 500 took 1.465662s\n",
      "   val loss: 0.061400\n",
      "   val acc: 0.982800\n",
      "Epoch 150 of 500 took 1.477048s\n",
      "   val loss: 0.060076\n",
      "   val acc: 0.983800\n",
      "Epoch 155 of 500 took 1.462218s\n",
      "   val loss: 0.059112\n",
      "   val acc: 0.983900\n",
      "Epoch 160 of 500 took 1.466274s\n",
      "   val loss: 0.059208\n",
      "   val acc: 0.983900\n",
      "Epoch 165 of 500 took 1.612575s\n",
      "   val loss: 0.058729\n",
      "   val acc: 0.983900\n",
      "Epoch 170 of 500 took 1.627474s\n",
      "   val loss: 0.058785\n",
      "   val acc: 0.984000\n",
      "Epoch 175 of 500 took 1.369380s\n",
      "   val loss: 0.058434\n",
      "   val acc: 0.984300\n",
      "Epoch 180 of 500 took 1.461449s\n",
      "   val loss: 0.057212\n",
      "   val acc: 0.985000\n",
      "Epoch 185 of 500 took 1.599898s\n",
      "   val loss: 0.057205\n",
      "   val acc: 0.984700\n",
      "Epoch 190 of 500 took 1.780936s\n",
      "   val loss: 0.057089\n",
      "   val acc: 0.985500\n",
      "Epoch 195 of 500 took 1.573079s\n",
      "   val loss: 0.057051\n",
      "   val acc: 0.984700\n",
      "Epoch 200 of 500 took 1.644199s\n",
      "   val loss: 0.057270\n",
      "   val acc: 0.984500\n",
      "Epoch 205 of 500 took 1.267548s\n",
      "   val loss: 0.056888\n",
      "   val acc: 0.984900\n",
      "Epoch 210 of 500 took 1.498259s\n",
      "   val loss: 0.055929\n",
      "   val acc: 0.985500\n",
      "Epoch 215 of 500 took 1.413578s\n",
      "   val loss: 0.055611\n",
      "   val acc: 0.985300\n",
      "Epoch 220 of 500 took 1.275742s\n",
      "   val loss: 0.056022\n",
      "   val acc: 0.985200\n",
      "Epoch 225 of 500 took 1.537626s\n",
      "   val loss: 0.054834\n",
      "   val acc: 0.986000\n",
      "Epoch 230 of 500 took 1.305988s\n",
      "   val loss: 0.055751\n",
      "   val acc: 0.985900\n",
      "Epoch 235 of 500 took 1.288111s\n",
      "   val loss: 0.055711\n",
      "   val acc: 0.985600\n",
      "Epoch 240 of 500 took 1.338327s\n",
      "   val loss: 0.055440\n",
      "   val acc: 0.985700\n",
      "Epoch 245 of 500 took 1.284250s\n",
      "   val loss: 0.054315\n",
      "   val acc: 0.985600\n",
      "Epoch 250 of 500 took 1.329624s\n",
      "   val loss: 0.055436\n",
      "   val acc: 0.985800\n",
      "Epoch 255 of 500 took 1.485040s\n",
      "   val loss: 0.055300\n",
      "   val acc: 0.985000\n",
      "Epoch 260 of 500 took 1.345345s\n",
      "   val loss: 0.055508\n",
      "   val acc: 0.986000\n",
      "Epoch 265 of 500 took 1.494656s\n",
      "   val loss: 0.055549\n",
      "   val acc: 0.985700\n",
      "Epoch 270 of 500 took 1.258638s\n",
      "   val loss: 0.055137\n",
      "   val acc: 0.985500\n",
      "Epoch 275 of 500 took 1.567175s\n",
      "   val loss: 0.055874\n",
      "   val acc: 0.986100\n",
      "Epoch 280 of 500 took 1.318266s\n",
      "   val loss: 0.054513\n",
      "   val acc: 0.986000\n",
      "Epoch 285 of 500 took 1.239516s\n",
      "   val loss: 0.054181\n",
      "   val acc: 0.986500\n",
      "Epoch 290 of 500 took 1.546228s\n",
      "   val loss: 0.055509\n",
      "   val acc: 0.985800\n",
      "Epoch 295 of 500 took 1.397402s\n",
      "   val loss: 0.055552\n",
      "   val acc: 0.985600\n",
      "Epoch 300 of 500 took 1.633413s\n",
      "   val loss: 0.056388\n",
      "   val acc: 0.986000\n",
      "Epoch 305 of 500 took 1.584159s\n",
      "   val loss: 0.055278\n",
      "   val acc: 0.986100\n",
      "Epoch 310 of 500 took 1.286382s\n",
      "   val loss: 0.055509\n",
      "   val acc: 0.986000\n",
      "Epoch 315 of 500 took 1.346385s\n",
      "   val loss: 0.055958\n",
      "   val acc: 0.986100\n",
      "Epoch 320 of 500 took 1.281425s\n",
      "   val loss: 0.056544\n",
      "   val acc: 0.985800\n",
      "Epoch 325 of 500 took 1.273356s\n",
      "   val loss: 0.055091\n",
      "   val acc: 0.986300\n",
      "Epoch 330 of 500 took 1.301316s\n",
      "   val loss: 0.056706\n",
      "   val acc: 0.985800\n",
      "Epoch 335 of 500 took 1.244685s\n",
      "   val loss: 0.055743\n",
      "   val acc: 0.985600\n",
      "Epoch 340 of 500 took 1.554135s\n",
      "   val loss: 0.055578\n",
      "   val acc: 0.986300\n",
      "Epoch 345 of 500 took 1.615196s\n",
      "   val loss: 0.055569\n",
      "   val acc: 0.986500\n",
      "Epoch 350 of 500 took 1.529667s\n",
      "   val loss: 0.055369\n",
      "   val acc: 0.986500\n",
      "Epoch 355 of 500 took 1.512177s\n",
      "   val loss: 0.056814\n",
      "   val acc: 0.986200\n",
      "Epoch 360 of 500 took 1.672079s\n",
      "   val loss: 0.054942\n",
      "   val acc: 0.986800\n",
      "Epoch 365 of 500 took 1.656595s\n",
      "   val loss: 0.055400\n",
      "   val acc: 0.987200\n",
      "Epoch 370 of 500 took 1.686935s\n",
      "   val loss: 0.055101\n",
      "   val acc: 0.987000\n",
      "Epoch 375 of 500 took 1.594482s\n",
      "   val loss: 0.056366\n",
      "   val acc: 0.986400\n",
      "Epoch 380 of 500 took 1.293605s\n",
      "   val loss: 0.054665\n",
      "   val acc: 0.986700\n",
      "Epoch 385 of 500 took 1.442632s\n",
      "   val loss: 0.055722\n",
      "   val acc: 0.986500\n",
      "Epoch 390 of 500 took 1.325401s\n",
      "   val loss: 0.055451\n",
      "   val acc: 0.986600\n",
      "Epoch 395 of 500 took 1.468976s\n",
      "   val loss: 0.055761\n",
      "   val acc: 0.987000\n",
      "Epoch 400 of 500 took 1.399271s\n",
      "   val loss: 0.055770\n",
      "   val acc: 0.986100\n",
      "Epoch 405 of 500 took 1.379668s\n",
      "   val loss: 0.056377\n",
      "   val acc: 0.986800\n",
      "Epoch 410 of 500 took 1.861037s\n",
      "   val loss: 0.055967\n",
      "   val acc: 0.986900\n",
      "Epoch 415 of 500 took 1.505245s\n",
      "   val loss: 0.055028\n",
      "   val acc: 0.986700\n",
      "Epoch 420 of 500 took 1.308357s\n",
      "   val loss: 0.056547\n",
      "   val acc: 0.986700\n",
      "Epoch 425 of 500 took 1.466472s\n",
      "   val loss: 0.056319\n",
      "   val acc: 0.986800\n",
      "Epoch 430 of 500 took 1.689827s\n",
      "   val loss: 0.055513\n",
      "   val acc: 0.987100\n",
      "Epoch 435 of 500 took 1.779764s\n",
      "   val loss: 0.055412\n",
      "   val acc: 0.987300\n",
      "Epoch 440 of 500 took 1.479461s\n",
      "   val loss: 0.056880\n",
      "   val acc: 0.986700\n",
      "Epoch 445 of 500 took 1.669860s\n",
      "   val loss: 0.055520\n",
      "   val acc: 0.987100\n",
      "Epoch 450 of 500 took 1.375352s\n",
      "   val loss: 0.056459\n",
      "   val acc: 0.986700\n",
      "Epoch 455 of 500 took 1.282485s\n",
      "   val loss: 0.057814\n",
      "   val acc: 0.986500\n",
      "Epoch 460 of 500 took 1.766598s\n",
      "   val loss: 0.056610\n",
      "   val acc: 0.987300\n",
      "Epoch 465 of 500 took 1.510641s\n",
      "   val loss: 0.057087\n",
      "   val acc: 0.987100\n",
      "Epoch 470 of 500 took 1.369138s\n",
      "   val loss: 0.056669\n",
      "   val acc: 0.987100\n",
      "Epoch 475 of 500 took 1.479506s\n",
      "   val loss: 0.056837\n",
      "   val acc: 0.986700\n",
      "Epoch 480 of 500 took 1.505226s\n",
      "   val loss: 0.058545\n",
      "   val acc: 0.986900\n",
      "Epoch 485 of 500 took 1.339965s\n",
      "   val loss: 0.057135\n",
      "   val acc: 0.987800\n",
      "Epoch 490 of 500 took 1.512697s\n",
      "   val loss: 0.058486\n",
      "   val acc: 0.986700\n",
      "Epoch 495 of 500 took 1.408185s\n",
      "   val loss: 0.058211\n",
      "   val acc: 0.987600\n",
      "Epoch 500 of 500 took 1.328156s\n",
      "   val loss: 0.058041\n",
      "   val acc: 0.986800\n",
      "Total training time: 740.893859s\n"
     ]
    }
   ],
   "source": [
    "tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_,\n",
    "             acc=acc, batch_size=500, n_epoch=500, print_freq=5,\n",
    "             X_val=X_val, y_val=y_val, eval_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We would like to use `tl.utils.test` to test a given non time-series network by the given test data and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing the network ...\n",
      "   test loss: 0.048938\n",
      "   test acc: 0.987900\n"
     ]
    }
   ],
   "source": [
    "tl.utils.test(sess, network, acc, X_test, y_test,\n",
    "              x, y_, batch_size=None, cost=cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the network to .npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] model.npz saved\n"
     ]
    }
   ],
   "source": [
    "tl.files.save_npz(network.all_params, name='model.npz')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit\n",
    "\n",
    "Credit goes to [TensorLayer Example](https://github.com/zsdonghao/tensorlayer/tree/master/example) for the majority of this code. I've merely create a jupyter notebook to make it more readable, and add some of my personal note.\n",
    "\n",
    "Due to my personal limited ability, if you have got any mistake, pleae don't hesitate to email at me(chengzehua@outlook.com).\n",
    "\n",
    "Thank you very much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
